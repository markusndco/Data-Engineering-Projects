ğŸ“Š Big Data Log Analytics Pipeline  

## ğŸ“– Overview  
This project demonstrates a **big data ETL pipeline** for processing and analyzing application logs at scale. Raw logs are ingested into an HDFS-style partitioned structure, processed with **PySpark**, stored as curated Parquet, loaded into **Amazon Redshift**, and queried using **Hive** for insights.  

---

## ğŸ¯ Objectives  
- âš¡ Ingest raw CSV logs into partitioned HDFS-like structure  
- ğŸ›  Process logs with PySpark to compute KPIs (users, events, latency)  
- ğŸ“‚ Store curated data in Parquet format for efficient querying  
- ğŸ—„ Load curated datasets into Amazon Redshift for analytics  
- ğŸ” Define Hive external tables for querying Parquet data  
- ğŸ“Š Enable log-based metrics for operational insights  

---

## ğŸ— Architecture  
- **Ingestion**: Python pipeline partitions raw logs by date and hour:contentReference[oaicite:0]{index=0}  
- **Processing**: PySpark cleans, aggregates, and writes curated Parquet KPIs:contentReference[oaicite:1]{index=1}  
- **Storage**: Parquet files stored in curated zone  
- **Warehouse**: Redshift loader copies curated data into analytics tables:contentReference[oaicite:2]{index=2}  
- **Query Layer**: Hive external tables enable BI/SQL access:contentReference[oaicite:3]{index=3}  

---

## ğŸŒŸ Features  
- Automated ingestion with partitioning  
- Spark-based data cleaning, validation, and aggregation  
- Storage optimization with Parquet  
- Redshift integration for analytical queries  
- Hive queries for ad-hoc analytics  
- End-to-end pipeline simulation of big data stack  

---

## ğŸ›  Tech Stack  
- **Ingestion:** Python (CSV â†’ partitioned HDFS style)  
- **Processing:** Apache Spark (PySpark)  
- **Storage:** Parquet files (curated zone)  
- **Warehouse:** Amazon Redshift  
- **Query Layer:** Apache Hive  

---

## ğŸ“‚ Repository Structure  
.
```
â”œâ”€â”€ README.md # Project overview
â”œâ”€â”€ ingestion_pipeline.py # Ingest raw CSV logs into partitioned folders
â”œâ”€â”€ spark_processing.py # PySpark job for cleaning & aggregations
â”œâ”€â”€ redshift_loader.py # Load curated data from S3 â†’ Redshift
â”œâ”€â”€ hive_queries.sql # Hive DDL & sample queries
â”œâ”€â”€ data/
â”‚ â””â”€â”€ sample_logs.csv # Example raw logs for pipeline demo
```

---

## ğŸš€ Getting Started  
1. ğŸ“¥ Place raw logs into `data/sample_logs.csv`  
2. ğŸŒ€ Run `ingestion_pipeline.py` to create partitioned raw data  
3. ğŸ”¥ Run `spark_processing.py` to generate curated Parquet outputs  
4. ğŸ—„ Use `redshift_loader.py` to load curated data into Redshift  
5. ğŸ” Execute `hive_queries.sql` to query curated Parquet tables  

---

## ğŸ” Use Cases  
- Application log analytics  
- Monitoring latency, errors, and usage patterns  
- Building KPI dashboards from log events  
- Demonstrating big data ETL workflows  

---

## ğŸ“œ License  
This project is provided for **educational and demonstration purposes**. Adapt for production with proper security, scalability, and compliance measures.  
