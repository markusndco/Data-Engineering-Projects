ğŸ“Š GCP Streaming Analytics Platform  

## ğŸ“– Overview  
This project implements a **streaming analytics platform on Google Cloud Platform (GCP)**. It ingests clickstream-like events, processes them with Databricks Structured Streaming, validates data with Great Expectations, and persists bronze/silver/gold layers in **BigQuery**. Infrastructure is provisioned via **Terraform**, monitoring is handled with Python alerting, and insights are surfaced through **Looker dashboards**.  

---

## ğŸ¯ Objectives  
- âš¡ Ingest raw events from GCS into Databricks using Auto Loader  
- ğŸ›  Transform events through **bronze â†’ silver â†’ gold** layers in BigQuery  
- âœ… Apply hourly **data quality validations** with Great Expectations  
- ğŸ–¥ Provision infrastructure with Terraform for repeatability  
- ğŸ“Š Deliver real-time insights with Looker dashboards  
- ğŸ”” Trigger monitoring alerts for anomalies or schema issues  

---

## ğŸ— Architecture  
- **Ingestion**: Raw event data lands in **Google Cloud Storage**  
- **ETL**: Databricks Structured Streaming parses, cleanses, deduplicates, and aggregates into bronze, silver, and gold BigQuery tables:contentReference[oaicite:0]{index=0}  
- **Data Quality**: Great Expectations validates bronze/silver/gold layers hourly:contentReference[oaicite:1]{index=1}  
- **Storage**: BigQuery serves as the analytical warehouse  
- **Monitoring**: Python-based alerting monitors data quality & anomalies  
- **Dashboards**: Looker visualizes KPIs from gold tables  
- **Infra**: Terraform defines GCP resources (buckets, datasets, IAM)  

---

## ğŸŒŸ Features  
- Scalable ingestion with Databricks Auto Loader  
- Exactly-once guarantees with checkpoints  
- Tiered data modeling: bronze â†’ silver â†’ gold  
- Automated Great Expectations data validation  
- Infrastructure-as-code with Terraform  
- Real-time alerting with Python monitoring scripts  
- Interactive Looker dashboards  

---

## ğŸ›  Tech Stack  
- **Compute & ETL**: Databricks, PySpark  
- **Storage & Warehouse**: Google Cloud Storage, BigQuery  
- **Data Quality**: Great Expectations  
- **Infra**: Terraform  
- **Monitoring**: Python (custom alerts)  
- **Visualization**: Looker  

---

## ğŸ“‚ Repository Structure 
``` 
â”œâ”€â”€ README.md # Project overview
â”œâ”€â”€ etl_pipeline.py # Databricks streaming ETL (bronze/silver/gold)
â”œâ”€â”€ data_quality_suite.py # Great Expectations validation suite
â”œâ”€â”€ monitoring_alerts.py # Monitoring and alerting script
â”œâ”€â”€ gcp_infra.tf # Terraform config for GCP infra
â”œâ”€â”€ looker_dashboard.json # Looker dashboard definition
â”œâ”€â”€ data/
â”‚ â””â”€â”€ event_stream_sample_small.csv # Sample event data
```

---

## ğŸš€ Getting Started  
1. ğŸ–¥ Provision GCP resources with Terraform (`gcp_infra.tf`)  
2. âš¡ Deploy ETL to Databricks (`etl_pipeline.py`) to process events into BigQuery  
3. âœ… Run data validations with Great Expectations (`data_quality_suite.py`)  
4. ğŸ”” Configure monitoring (`monitoring_alerts.py`) for anomalies  
5. ğŸ“Š Import Looker dashboard (`looker_dashboard.json`) for analytics  

---

## ğŸ” Use Cases  
- Real-time **event stream analytics**  
- **Data quality monitoring** in streaming pipelines  
- **Product analytics** dashboards (sessions, revenue, anomalies)  
- Scalable, cloud-native ETL architecture  

---

## ğŸ“œ License  
This project is provided for **educational and demonstration purposes**. Please review compliance and cost considerations before production deployment.  
